"""
@author: Zahra
"""
from __future__ import print_function
import os
os.environ['CUDA_VISIBLE_DEVICES'] = "0"
from keras.layers import *
import numpy as np
from keras.models import Model
import keras.backend as K
import keras
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def final_model(train, test, y_train, y_test, y_train_cat, y_test_cat,epochs,batch_size,opt):
    input = Input(shape=(32,))
    activations = Conv1D(32,
                         5,
                         strides=1,
                         padding='valid',
                         activation='relu',
                         kernel_initializer='random_normal',
                         use_bias=True)(input)
    activations = MaxPooling1D(pool_size=3)(activations)
    activations = Dropout(0.35)(activations)
    activations = Conv1D(64, 5, activation='tanh')(activations)
    activations = MaxPool1D(pool_size=3)(activations)
    activations = Dropout(0.45)(activations)
    activations = Bidirectional(LSTM(64, activation='tanh', return_sequences=True))(activations)
    activations = Dropout(0.35)(activations)
    # define attention mechanism
    attention = TimeDistributed(Dense(1, activation='tanh'))(activations)
    attention = Flatten()(attention)
    attention = Activation('softmax')(attention)
    attention = RepeatVector(128)(attention)
    attention = Permute([2, 1])(attention)
    # apply the attention
    multiply_layer = keras.layers.Multiply()
    sent_representation = multiply_layer([activations, attention])
    sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(128,))(sent_representation)
    probabilities = Dense(2, activation='softmax')(sent_representation)
    model = Model(input=input, output=probabilities)
    model.compile(
        optimizer=opt,
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    model.summary()
    history = model.fit(train, y_train_cat,
                        batch_size=batch_size,
                        epochs=epochs,
                        verbose=1,
                        validation_data=(test, y_test_cat))

    # Plot training & validation accuracy values
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

    # Plot training & validation loss values
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

    y_pred = model.predict(test, verbose=1)
    ypreds = np.argmax(y_pred, axis=1)
    print(accuracy_score(y_test, ypreds))

    return accuracy_score(y_test, ypreds)
